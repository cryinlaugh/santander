{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "#load origin data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "lines = np.loadtxt('./sample_submission.csv',delimiter=',', dtype=str)\n",
    "data = lines[1:].astype(int)\n",
    "\n",
    "lines = np.loadtxt('./train.csv', delimiter=',', dtype=str)\n",
    "[nrow, ncol] = lines.shape\n",
    "print nrow, ncol\n",
    "#lines[0]\n",
    "train_id = lines[1:, 0].astype(int)\n",
    "train_target = lines[1:, ncol-1].astype(int)\n",
    "train_data_ori = lines[1:, 1:ncol-1].astype(float)\n",
    "feature_names = lines[0,:]\n",
    "\n",
    "#read test data\n",
    "lines = np.loadtxt('./test.csv', delimiter=',', dtype=str)\n",
    "test_id = lines[1:, 0].astype(int)\n",
    "test_data_ori = lines[1:, 1:ncol-1].astype(float)\n",
    "print 'Done load train&test data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "#delete single-value features\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "varThresProb = 1.\n",
    "sel = VarianceThreshold(varThresProb*(1-varThresProb))\n",
    "train_data = sel.fit_transform(train_data_ori)\n",
    "\n",
    "#MODEL PARAM\n",
    "param_selected_indices = sel.get_support(indices=True)\n",
    "\n",
    "print 'Done delete redundant features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 337)\n",
      "(76020, 308)\n",
      "(76020, 306)\n",
      "(75818, 306)\n"
     ]
    }
   ],
   "source": [
    "#load data and delete redundant data\n",
    "\n",
    "remove = []\n",
    "\n",
    "for col in df_train:\n",
    "    if df_train[col].std() == 0:\n",
    "        remove.append(col)\n",
    "\n",
    "df_train.drop(remove, axis=1, inplace=True)\n",
    "df_test.drop(remove, axis=1, inplace=True)\n",
    "print (df_train.values).shape\n",
    "\n",
    "remove = []\n",
    "for i in range(df_train.shape[1]):\n",
    "    for j in range(i+1, df_train.shape[1]):\n",
    "        if np.array_equal(df_train[df_train.columns[i]].values,df_train[df_train.columns[j]].values):\n",
    "            remove.append(df_train.columns[j])\n",
    "df_train.drop(remove, axis=1, inplace=True)\n",
    "df_test.drop(remove, axis=1, inplace=True)\n",
    "\n",
    "print (df_train.values).shape\n",
    "\n",
    "train_data_id = df_train['ID'].values\n",
    "test_data_id = df_test['ID'].values\n",
    "#print train_data_id.shape\n",
    "\n",
    "train_data_target = df_train['TARGET'].values\n",
    "#print train_data_target.shape\n",
    "\n",
    "train_data = df_train.drop(['ID', 'TARGET'],axis=1).values\n",
    "test_data = df_test.drop(['ID'],axis=1).values\n",
    "print train_data.shape\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 bool, 113 int and 110 float\n"
     ]
    }
   ],
   "source": [
    "#training data type analyses\n",
    "\n",
    "def type_analyses(nums):\n",
    "    data_type = 1 #int\n",
    "    for num in nums:\n",
    "        if(round(float(num)) != float(num)): #float\n",
    "            data_type = 2\n",
    "    if (data_type == 1) & (nums.shape[0]==2):\n",
    "        data_type = 0\n",
    "    return data_type\n",
    "\n",
    "unique_data = []\n",
    "unique_data_size = []\n",
    "unique_data_type = []\n",
    "typeName = ['bool', 'int', 'float']\n",
    "\n",
    "for i in range(train_data.shape[1]):\n",
    "    values = np.unique(train_data[:,i])\n",
    "    unique_data.append(values)\n",
    "    unique_data_size.append(values.shape)\n",
    "    feature_type = type_analyses(values)\n",
    "    unique_data_type.append(feature_type)\n",
    "\n",
    "\n",
    "param_bool_feature_indices = np.argwhere(np.array(unique_data_type)==0).reshape(unique_data_type.count(0))\n",
    "param_int_feature_indices = np.argwhere(np.array(unique_data_type)==1).reshape(unique_data_type.count(1))\n",
    "param_float_feature_indices = np.argwhere(np.array(unique_data_type)==2).reshape(unique_data_type.count(2))\n",
    "\n",
    "print '%d bool, %d int and %d float' % (unique_data_type.count(0), \n",
    "                                        unique_data_type.count(1), \n",
    "                                        unique_data_type.count(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions def\n",
    "#for train&test dataset preprocessing\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "from sklearn.preprocessing import robust_scale\n",
    "\n",
    "#preprocess alg\n",
    "\n",
    "def get_range_bool(train_data_bool):\n",
    "    list_range_bool = []\n",
    "    for i in range(train_data_bool.shape[1]):\n",
    "        values = np.unique(train_data_bool[:,i])\n",
    "        vzero = np.min(values)\n",
    "        vone = np.max(values)\n",
    "        list_range_bool.append([vzero,vone])\n",
    "    return np.transpose(np.array(list_range_bool))\n",
    "\n",
    "def get_range_int (train_data_int):\n",
    "    #check int value range    \n",
    "    list_range_int = []\n",
    "    for i in range(train_data_int.shape[1]):\n",
    "        list_range_int.append([np.min(train_data_int[:,i]),np.max(train_data_int[:,i])])\n",
    "    return np.transpose(np.array(list_range_int))\n",
    "\n",
    "def scale_float(float_features_ori):\n",
    "    return maxabs_scale(float_features_ori, axis=0)\n",
    "\n",
    "def scale_bool (bool_features_ori, bool_feature_range):\n",
    "    bool_features = np.asarray(bool_features_ori)\n",
    "    for i in range(bool_features.shape[0]):\n",
    "        for j in range(bool_features.shape[1]):\n",
    "            if bool_features[i,j]==bool_feature_range[0,j]:\n",
    "                bool_features[i,j] = 0\n",
    "            else:\n",
    "                bool_features[i,j] = 1\n",
    "    ##old version\n",
    "    #bool_features = bool_features_ori - np.min(bool_features_ori, axis=0).reshape(1, bool_features_ori.shape[1])\n",
    "    #bool_features = bool_features / np.max(bool_features, axis=0).reshape(1,bool_features.shape[1])\n",
    "    return bool_features\n",
    "\n",
    "def scale_int (int_features_ori, int_feature_range):\n",
    "    int_features = np.array(int_features_ori)\n",
    "    for i in range(int_features.shape[1]):\n",
    "        pos_neg_feature = -2 #pos+neg\n",
    "        if (int_feature_range[0,i] <0) & (int_feature_range[1, i] <=0):\n",
    "            pos_neg_feature = -1\n",
    "        if (int_feature_range[0,i] <0 ) & (int_feature_range[1, i] >0):\n",
    "            pos_neg_feature = 0\n",
    "        if (int_feature_range[0,i] >=0) & (int_feature_range[1, i] >0):\n",
    "            pos_neg_feature = 1\n",
    "\n",
    "        #scale\n",
    "        for j in range(int_features.shape[0]):\n",
    "            if int_features[j,i] < int_feature_range[0,i]:\n",
    "                int_features[j,i] = int_feature_range[0,i];\n",
    "            if int_features[j,i] > int_feature_range[1,i]:\n",
    "                int_features[j,i] = int_feature_range[1,i];\n",
    "            if pos_neg_feature == -1:\n",
    "                int_features[j,i] = int_features[j,i]/-int_feature_range[0,i]\n",
    "                if int_features[j,i] < -1:\n",
    "                    int_features[j,i] = -1.0\n",
    "            if pos_neg_feature == 0:\n",
    "                if int_features[j,i] < 0:\n",
    "                    int_features[j,i] = int_features[j,i]/-int_feature_range[0,i]\n",
    "                    if int_features[j,i] < -1:\n",
    "                        int_features[j,i] = -1.0\n",
    "                else:\n",
    "                    int_features[j,i] = int_features[j,i]/int_feature_range[1,i]\n",
    "                    if int_features[j,i] > 1:\n",
    "                        int_features[j,i] = 1.0\n",
    "            if pos_neg_feature == 1:\n",
    "                int_features[j,i] = int_features[j,i]/int_feature_range[1,i]\n",
    "                if int_features[j,i] > 1:\n",
    "                    int_features[j,i] = 1.0\n",
    "    return int_features\n",
    "\n",
    "#debug alg\n",
    "def printarray(data):\n",
    "    print data.shape\n",
    "    print data\n",
    "    #print np.unique(data)\n",
    "    #print [np.sum(data==i) for i in np.unique(data)]\n",
    "    #print np.unique(scale(test_data))\n",
    "    #print np.unique(maxabs_scale(test_data))\n",
    "    #print np.unique(robust_scale(test_data))\n",
    "    #print np.sum(test_data==np.unique(test_data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00840336  0.21904762  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.00840336  0.32380952  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.00840336  0.21904762  0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.00840336  0.21904762  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.00840336  0.23809524  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.00840336  0.43809524  0.         ...,  0.          0.          0.        ]]\n",
      "(76020, 306)\n",
      "(75818, 306)\n",
      "Done type analysis and scale!\n"
     ]
    }
   ],
   "source": [
    "#scale data based on type\n",
    "\n",
    "\n",
    "do_type_part = True\n",
    "\n",
    "if do_type_part:\n",
    "    train_data_bool = train_data[:,param_bool_feature_indices]\n",
    "    train_data_int = train_data[:, param_int_feature_indices]\n",
    "    train_data_float = train_data[:, param_float_feature_indices]\n",
    "    \n",
    "    #test data preprocessing\n",
    "    test_data_bool = test_data[:,param_bool_feature_indices]\n",
    "    test_data_int = test_data[:, param_int_feature_indices]\n",
    "    test_data_float = test_data[:, param_float_feature_indices]\n",
    "    \n",
    "    ##Generate MODEL PARAM\n",
    "    param_bool_feature_range = get_range_bool(train_data_bool)\n",
    "    #scale bool feature\n",
    "    train_data_bool_scale = scale_bool(train_data_bool, param_bool_feature_range)\n",
    "    test_data_bool_scale = scale_bool(test_data_bool, param_bool_feature_range)\n",
    "    \n",
    "    #Generate MODEL PARAM\n",
    "    param_int_feature_range = get_range_int(train_data_int)\n",
    "    #scale int feature:\n",
    "    train_data_int_scale = scale_int(train_data_int, param_int_feature_range)\n",
    "    test_data_int_scale = scale_int(test_data_int, param_int_feature_range)\n",
    "    \n",
    "    print train_data_int_scale\n",
    "    \n",
    "    #scale float feaure:\n",
    "    data_float_scale = scale_float(np.row_stack((train_data_float, test_data_float)))\n",
    "    train_data_float_scale = data_float_scale[:train_data_float.shape[0], :]\n",
    "    test_data_float_scale = data_float_scale[train_data_float.shape[0] : , :]\n",
    "    \n",
    "    #printarray(train_data_float_scale[:,0])\n",
    "    train_data_scale = np.column_stack((train_data_bool_scale, train_data_int_scale, train_data_float_scale))\n",
    "    test_data_scale = np.column_stack((test_data_bool_scale, test_data_int_scale, test_data_float_scale))\n",
    "    \n",
    "    #train_data_scale = np.column_stack((train_data_bool, train_data_int, train_data_float))\n",
    "    #test_data_scale = np.column_stack((test_data_bool, test_data_int, test_data_float))\n",
    "    \n",
    "    print train_data_scale.shape\n",
    "    print test_data_scale.shape\n",
    "    \n",
    "print \"Done type analysis and scale!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = df_train.columns\n",
    "#print cols[1:cols.shape[0]-1]\n",
    "#print df_train.loc[:,cols[1:cols.shape[0]-1]]\n",
    "df_train.loc[:,cols[1:cols.shape[0]-1]] = train_data_scale\n",
    "df_test.loc[:,cols[1:cols.shape[0]-1]] = test_data_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 308)\n",
      "(75818, 307)\n"
     ]
    }
   ],
   "source": [
    "df_train.to_csv('./data/train_data_scale.csv', index=False)\n",
    "df_test.to_csv('./data/test_data_scale.csv', index=False)\n",
    "print df_train.shape\n",
    "print df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#save scaled train/test data\n",
    "\n",
    "#save param_* \n",
    "import csv\n",
    "with open('data_processing_param.csv', 'w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(param_selected_indices)\n",
    "    f_csv.writerow(param_bool_feature_indices)\n",
    "    f_csv.writerow(param_int_feature_indices)\n",
    "    f_csv.writerow(param_float_feature_indices)\n",
    "    f_csv.writerows(param_bool_feature_range)\n",
    "    f_csv.writerows(param_int_feature_range)\n",
    "    \n",
    "    \n",
    "#save header + scaled data\n",
    "headers = np.ndarray((param_selected_indices.shape[0]+2),dtype=feature_names.dtype)\n",
    "headers[1:param_selected_indices.shape[0]+1] = feature_names[param_selected_indices+1]\n",
    "headers[0] = feature_names[0]\n",
    "headers[param_selected_indices.shape[0]+1] = feature_names[feature_names.shape[0]-1]\n",
    "data_scale_id_target = np.column_stack((train_id, train_data_scale, train_target))\n",
    "print headers.shape\n",
    "print data_scale_id_target.shape\n",
    "with open('train_data_scale.csv', 'w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerows(data_scale_id_target)\n",
    "print 'Done saving scaled training data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#test data read/select/scale\n",
    "\n",
    "#load params from taining data analysis\n",
    "import csv\n",
    "with open('./data_processing_param.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    params = []\n",
    "    for row in f_csv:\n",
    "        params.append(np.array(row).astype(float))\n",
    "param_selected_indices = params[0].astype(int)\n",
    "param_bool_feature_indices = params[1].astype(int)\n",
    "param_int_feature_indices = params[2].astype(int)\n",
    "param_float_feature_indices = params[3].astype(int)\n",
    "param_bool_feature_range = np.asarray(np.row_stack((params[4],params[5])))\n",
    "param_int_feature_range = np.asarray(np.row_stack((params[6],params[7])))\n",
    "\n",
    "#select features\n",
    "test_data = test_data_ori[:,param_selected_indices.astype(int)]\n",
    "test_data_bool = test_data[:,param_bool_feature_indices]\n",
    "test_data_int = test_data[:,param_int_feature_indices]\n",
    "test_data_float = test_data[:,param_float_feature_indices]\n",
    "\n",
    "#scale bool features\n",
    "test_data_bool_scale = scale_bool(test_data_bool, param_bool_feature_range)\n",
    "test_data_int_scale = scale_int(test_data_int, param_int_feature_range)\n",
    "test_data_float_scale = scale_float(test_data_float)\n",
    "\n",
    "test_data_scale = np.column_stack((test_data_bool_scale, test_data_int_scale, test_data_float_scale))\n",
    "\n",
    "#printarray(test_data_scale)\n",
    "print 'Done scale test data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 308)\n",
      "(76020, 306)\n",
      "(76020, 306)\n",
      "(75818, 306)\n"
     ]
    }
   ],
   "source": [
    "#training start load preprocessed data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train_scale = pd.read_csv('./data/train_data_scale.csv')\n",
    "df_test_scale = pd.read_csv('./data/test_data_scale.csv')\n",
    "\n",
    "\n",
    "\n",
    "#df_train_scale.replace(-99999, 2)\n",
    "#df_test_scale.replace(-99999, 2)\n",
    "\n",
    "train_data_target = df_train_scale['TARGET'].values\n",
    "test_data_id = df_test_scale['ID'].values\n",
    "\n",
    "train_data_scale = df_train_scale.drop(['ID','TARGET'], axis=1).values\n",
    "test_data_scale = df_test_scale.drop(['ID'], axis=1).values\n",
    "\n",
    "print df_train_scale.shape\n",
    "print df_train_scale.drop(['ID', 'TARGET'], axis=1).shape\n",
    "\n",
    "print train_data_scale.shape\n",
    "print test_data_scale.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00135684,  0.00170545,  0.00233034, ...,  0.00256202,\n",
       "        0.00291676,  0.00405999])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_scale[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020,)\n",
      "(76020, 2)\n",
      "(53214, 306)\n",
      "(22806, 306)\n",
      "(53214, 2)\n",
      "(22806, 2)\n"
     ]
    }
   ],
   "source": [
    "#Boosting \n",
    "#ref: https://www.kaggle.com/cast42/santander-customer-satisfaction/xgboost-with-early-stopping/code\n",
    "\n",
    "from sklearn import cross_validation as cv\n",
    "\n",
    "print train_data_target.shape\n",
    "\n",
    "# train_data_scale_n = np.row_stack((train_data_scale, train_data_scale[(train_data_target==1),:]))\n",
    "# train_data_target_n = np.hstack((train_data_target, train_data_target[train_data_target==1]))\n",
    "\n",
    "train_data_y = train_data_target.repeat(2).reshape([train_data_target.shape[0],2])\n",
    "print train_data_y.shape\n",
    "for i in xrange(train_data_y.shape[0]):\n",
    "    train_data_y[i,1] = 1-train_data_y[i,0]\n",
    "\n",
    "#x_train, x_valid, y_train, y_valid = \\\n",
    "#cv.train_test_split(train_data_scale, train_data_target, test_size=0.3, stratify=train_data_target)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = \\\n",
    "cv.train_test_split(train_data_scale, train_data_y, test_size=0.3, stratify=train_data_target)\n",
    "\n",
    "print x_train.shape\n",
    "print x_valid.shape\n",
    "print y_train.shape\n",
    "print y_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106\n",
      "51108\n",
      "902\n",
      "21904\n",
      "(53214, 306)\n",
      "(53214, 2)\n",
      "[[ 0.          0.          1.         ...,  0.          0.          0.00116276]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.00142706]\n",
      " [ 0.          0.          1.         ...,  0.          0.          0.00673883]\n",
      " ..., \n",
      " [ 0.          0.          1.         ...,  0.          0.          0.00146538]\n",
      " [ 0.          0.          1.         ...,  0.          0.          0.00357552]\n",
      " [ 0.          0.          1.         ...,  0.          0.          0.00243501]]\n"
     ]
    }
   ],
   "source": [
    "print np.sum(y_train[:,1]==0)\n",
    "print np.sum(y_train[:,1]==1)\n",
    "print np.sum(y_valid[:,1]==0)\n",
    "print np.sum(y_valid[:,1]==1)\n",
    "x_train = x_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "x_valid = x_valid.astype(np.float32)\n",
    "y_valid = y_valid.astype(np.float32)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Train set ROC: 0.720097, Valid set ROC: 0.718136\n",
      "epoch 1\n",
      "Train set ROC: 0.744322, Valid set ROC: 0.737277\n",
      "epoch 2\n",
      "Train set ROC: 0.757180, Valid set ROC: 0.746223\n",
      "epoch 3\n",
      "Train set ROC: 0.762470, Valid set ROC: 0.751574\n",
      "epoch 4\n",
      "Train set ROC: 0.765548, Valid set ROC: 0.755340\n",
      "epoch 5\n",
      "Train set ROC: 0.767423, Valid set ROC: 0.757342\n",
      "epoch 6\n",
      "Train set ROC: 0.769257, Valid set ROC: 0.758795\n",
      "epoch 7\n",
      "Train set ROC: 0.770660, Valid set ROC: 0.759905\n",
      "epoch 8\n",
      "Train set ROC: 0.771852, Valid set ROC: 0.760737\n",
      "epoch 9\n",
      "Train set ROC: 0.772729, Valid set ROC: 0.761482\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, -np.sqrt(6.0/(np.sum(shape))), np.sqrt(6.0/(np.sum(shape))))\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.0, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "network_layer = 2\n",
    "network_shape = [306, 150, 2]\n",
    "\n",
    "x_in = tf.placeholder(tf.float32, shape = [None, network_shape[0]])\n",
    "y_out = tf.placeholder(tf.float32, shape = [None, network_shape[network_layer]])\n",
    "\n",
    "#W_mlp1 = tf.Variable(tf.random_uniform([306, 500], -1.0, 1.0))\n",
    "#b_mlp1 = tf.Variable(tf.zeros([500]))\n",
    "\n",
    "#W_mlp2 = tf.Variable(tf.random_uniform())\n",
    "#b_mlp2 = \n",
    "\n",
    "W_mlp1 = weight_variable(network_shape[0:2])\n",
    "b_mlp1 = bias_variable([network_shape[1]])\n",
    "\n",
    "#W_mlp2 = weight_variable([500, 100])\n",
    "#b_mlp2 = bias_variable([100])\n",
    "\n",
    "W_mlp3 = weight_variable(network_shape[1:3])\n",
    "b_mlp3 = bias_variable([network_shape[network_layer]])\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# print sess.run(W)\n",
    "# print sess.run(b)\n",
    "\n",
    "out_mlp1 = tf.nn.tanh(tf.matmul(x_in, W_mlp1)+b_mlp1)\n",
    "#out_mlp2 = tf.nn.tanh(tf.matmul(out_mlp1, W_mlp2)+b_mlp2)\n",
    "\n",
    "y_pred = tf.nn.softmax(tf.matmul(out_mlp1, W_mlp3)+b_mlp3)\n",
    "\n",
    "# y_pred_train = sess.run(out_mlp1, feed_dict={x_in:x_train[0:2,...]})\n",
    "\n",
    "# print y_pred_train\n",
    "\n",
    "# y_pred_train = sess.run(y_pred, feed_dict={x_in:x_train[0:2,...]})\n",
    "\n",
    "# print y_pred_train\n",
    "\n",
    "# print np.sum(y_pred_train[:,0]==0)\n",
    "# print np.sum(y_pred_train[:,0]==1)\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_out*tf.log(y_pred))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.005).minimize(cross_entropy)\n",
    "\n",
    "batchsize = 50\n",
    "batchnum = int(x_train.shape[0]/batchsize)\n",
    "epoch = 10\n",
    "for i in xrange(epoch):\n",
    "    #print 'epoch %d, batch %d/%d' % (i, j, batchnum)\n",
    "    print 'epoch %d' % (i)\n",
    "    for j in xrange(batchnum):\n",
    "        \n",
    "        x_batch = x_train[j*batchsize:(j+1)*batchsize,:]\n",
    "        y_batch = y_train[j*batchsize:(j+1)*batchsize,:]\n",
    "        sess.run(train_step, feed_dict={x_in: x_batch, y_out: y_batch})\n",
    "#     print sess.run(W)\n",
    "#     print sess.run(b)\n",
    "    y_pred_train = sess.run(y_pred, feed_dict={x_in: x_train})\n",
    "    y_pred_valid = sess.run(y_pred, feed_dict={x_in: x_valid})\n",
    "    print \"Train set ROC: %f, Valid set ROC: %f\" % (roc_auc_score(y_train[:,0], y_pred_train[:,0]), \\\n",
    "                                                   roc_auc_score(y_valid[:,0], y_pred_valid[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ##[166]\tvalidation_0-auc:0.883586\tvalidation_1-auc:0.833347\n",
    "# ##('Overall AUC:', 0.86816240395414446)\n",
    "# ##test score: 0.830436\n",
    "# clf = xgb.XGBClassifier(missing=9999999999,\n",
    "#                 max_depth = 8,\n",
    "#                 n_estimators=1000,\n",
    "#                 learning_rate=0.05, \n",
    "#                 nthread=4,\n",
    "#                 subsample=0.8,\n",
    "#                 colsample_bytree=0.5,\n",
    "#                 min_child_weight = 8,\n",
    "#                 seed=4242)\n",
    "\n",
    "# #('Overall AUC:', 0.87137364469597534)\n",
    "# #Stopping. Best iteration:\n",
    "# #[150]\tvalidation_0-auc:0.887866\tvalidation_1-auc:0.833577\n",
    "# clf = xgb.XGBClassifier(missing=9999999999,\n",
    "#                 max_depth = 9,\n",
    "#                 n_estimators=1000,\n",
    "#                 learning_rate=0.05, \n",
    "#                 nthread=4,\n",
    "#                 subsample=0.8,\n",
    "#                 colsample_bytree=0.5,\n",
    "#                 min_child_weight = 8,\n",
    "#                 seed=4242)\n",
    "\n",
    "# ##('Overall AUC:', 0.8721795864254609)\n",
    "# ##Stopping. Best iteration:\n",
    "# ##[131]\tvalidation_0-auc:0.889038\tvalidation_1-auc:0.833293\n",
    "# clf = xgb.XGBClassifier(missing=9999999999,\n",
    "#                 max_depth = 10,\n",
    "#                 n_estimators=1000,\n",
    "#                 learning_rate=0.05, \n",
    "#                 nthread=4,\n",
    "#                 subsample=0.8,\n",
    "#                 colsample_bytree=0.5,\n",
    "#                 min_child_weight = 8,\n",
    "#                 seed=4242)\n",
    "\n",
    "\n",
    "##('Overall AUC:', 0.88376225142894027)\n",
    "##[203]\tvalidation_0-auc:0.912779\tvalidation_1-auc:0.832430\n",
    "##Stopping. Best iteration:\n",
    "##[153]\tvalidation_0-auc:0.905169\tvalidation_1-auc:0.833714\n",
    "###double class 1 sample\n",
    "##('Overall AUC:', 0.9541665781805323)\n",
    "##[999]\tvalidation_0-auc:0.972681\tvalidation_1-auc:0.907279\n",
    "# clf = xgb.XGBClassifier(missing=9999999999,\n",
    "#                 max_depth = 10,\n",
    "#                 n_estimators=1000,\n",
    "#                 learning_rate=0.05, \n",
    "#                 nthread=4,\n",
    "#                 subsample=0.8,\n",
    "#                 colsample_bytree=0.5,\n",
    "#                 min_child_weight = 5,\n",
    "#                 seed=4242)\n",
    "\n",
    "##('Overall AUC:', 0.89180781297900902)\n",
    "##Stopping. Best iteration:\n",
    "##[166]\tvalidation_0-auc:0.916351\tvalidation_1-auc:0.835339\n",
    "\n",
    "clf = xgb.XGBClassifier(missing=9999999999,\n",
    "                max_depth = 12,\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.05, \n",
    "                nthread=4,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.5,\n",
    "                min_child_weight = 8,\n",
    "                seed=1030)\n",
    "\n",
    "clf.fit(x_train, y_train, early_stopping_rounds=70, eval_metric=\"auc\",\n",
    "        eval_set=[(x_train, y_train), (x_test, y_test)])\n",
    "        \n",
    "print('Overall AUC:', roc_auc_score(train_data_target, clf.predict_proba(train_data_scale, ntree_limit=clf.best_iteration)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "y_pred = clf.predict_proba(test_data_scale, ntree_limit=clf.best_iteration)\n",
    "#print y_pred\n",
    "submission = pd.DataFrame({\"ID\":test_data_id, \"TARGET\":y_pred[:,1]})\n",
    "submission.to_csv(\"submission_nofloat_noint.csv\", index=False)\n",
    "\n",
    "y_pred_class = clf.predict(test_data_scale, ntree_limit=clf.best_iteration)\n",
    "print np.unique(y_pred_class)\n",
    "print np.sum(y_pred_class == 0)\n",
    "print np.sum(y_pred_class == 1)\n",
    "#submission_class = submission = pd.DataFrame({\"ID\":test_data_id, \"TARGET\":y_pred_class})\n",
    "#submission_class.to_csv('submission_class.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the wrong answers\n",
    "train_pred_target = clf.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pca analyses\n",
    "#ref:https://www.kaggle.com/tuomastik/santander-customer-satisfaction/pca-visualization/code\n",
    "\n",
    "#data normalization\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "do_l2_norm = True\n",
    "\n",
    "#train_data normalization\n",
    "if do_l2_norm:\n",
    "    train_data_norm = normalize(train_data_scale, axis=0)\n",
    "else:\n",
    "    train_data_norm = train_data_scale\n",
    "#print np.sum(train_data_norm[:,:2], axis=0)\n",
    "pca = PCA(n_components=2)\n",
    "train_data_pca = pca.fit_transform(train_data_norm)\n",
    "\n",
    "print \"Done PCA analysis with components = 2\"\n",
    "\n",
    "\n",
    "#plot\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "classes = np.sort(np.unique(train_target))\n",
    "labels = [\"Satisfied customer\", \"Unsatisfied customer\"]\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "colors = [(0.0, 0.63, 0.69), 'black']\n",
    "markers = [\"o\", \"D\"]\n",
    "print type(zip(classes, markers, colors, train_target))\n",
    "print zip(classes, markers, colors, train_target)\n",
    "for class_ix, marker, color, label in zip(\n",
    "        classes, markers, colors, labels):\n",
    "    print 'll: '+ str(class_ix) + str(marker) + str(color) + str(label)\n",
    "    ax.scatter(train_data_pca[train_target==class_ix, 0],\n",
    "               train_data_pca[train_target==class_ix, 1],\n",
    "               c=color, marker=marker, edgecolor='whitesmoke', \n",
    "               linewidth='1', alpha=0.9, label=label)\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "plt.title(\n",
    "    \"Scatter plot of the training data examples projected on the \"\n",
    "    \"2 first principal components\")\n",
    "plt.xlabel(\"Principal axis 1 - Explains %.1f %% of the variance\" % (\n",
    "    pca.explained_variance_ratio_[0] * 100.0))\n",
    "plt.ylabel(\"Principal axis 2 - Explains %.1f %% of the variance\" % (\n",
    "    pca.explained_variance_ratio_[1] * 100.0))\n",
    "plt.show()\n",
    "#plt.savefig(\"./pca.pdf\", format='pdf')\n",
    "#plt.savefig(\"./pca.png\", format='png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for dev, use 1 batch\n",
    "batch_size = 100\n",
    "train_data_b = train_data[:100]\n",
    "\n",
    "print nsample\n",
    "print nfeature\n",
    "\n",
    "for class_ix, marker, color, label in zip(\n",
    "        classes, markers, colors, train_target):\n",
    "    ax.scatter(train_data_pca[np.where(train_target == class_ix), 0],\n",
    "               train_data_pca[np.where(train_target == class_ix), 1],\n",
    "               marker=marker, color=color, edgecolor='whitesmoke',\n",
    "               linewidth='1', alpha=0.9, label=label)\n",
    "    ax.legend(loc='best')\n",
    "plt.title(\n",
    "    \"Scatter plot of the training data examples projected on the \"\n",
    "    \"2 first principal components\")\n",
    "plt.xlabel(\"Principal axis 1 - Explains %.1f %% of the variance\" % (\n",
    "    pca.explained_variance_ratio_[0] * 100.0))\n",
    "plt.ylabel(\"Principal axis 2 - Explains %.1f %% of the variance\" % (\n",
    "    pca.explained_variance_ratio_[1] * 100.0))\n",
    "plt.show()\n",
    "plt.savefig(\"pca.pdf\", format='pdf')\n",
    "plt.savefig(\"pca.png\", format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
